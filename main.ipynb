{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4830a71d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archticture:  Deep_Emotion(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "===================================Start Training===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AR\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AR\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.01299020 \tValidation Loss 0.01327502 \tTraining Acuuarcy 34.306% \tValidation Acuuarcy 38.479%\n",
      "Epoch: 2 \tTraining Loss: 0.01192511 \tValidation Loss 0.01195446 \tTraining Acuuarcy 40.681% \tValidation Acuuarcy 42.129%\n",
      "Epoch: 3 \tTraining Loss: 0.01122526 \tValidation Loss 0.01187934 \tTraining Acuuarcy 44.616% \tValidation Acuuarcy 42.073%\n",
      "Epoch: 4 \tTraining Loss: 0.01083722 \tValidation Loss 0.01161433 \tTraining Acuuarcy 46.721% \tValidation Acuuarcy 43.801%\n",
      "Epoch: 5 \tTraining Loss: 0.01047805 \tValidation Loss 0.01143534 \tTraining Acuuarcy 48.678% \tValidation Acuuarcy 45.305%\n",
      "Epoch: 6 \tTraining Loss: 0.01032897 \tValidation Loss 0.01099268 \tTraining Acuuarcy 49.158% \tValidation Acuuarcy 47.729%\n",
      "Epoch: 7 \tTraining Loss: 0.01014039 \tValidation Loss 0.01098418 \tTraining Acuuarcy 50.251% \tValidation Acuuarcy 47.562%\n",
      "Epoch: 8 \tTraining Loss: 0.01002601 \tValidation Loss 0.01069863 \tTraining Acuuarcy 50.848% \tValidation Acuuarcy 47.646%\n",
      "Epoch: 9 \tTraining Loss: 0.00992953 \tValidation Loss 0.01063256 \tTraining Acuuarcy 51.406% \tValidation Acuuarcy 49.122%\n",
      "Epoch: 10 \tTraining Loss: 0.00978839 \tValidation Loss 0.01076386 \tTraining Acuuarcy 51.889% \tValidation Acuuarcy 49.206%\n",
      "Epoch: 11 \tTraining Loss: 0.00971964 \tValidation Loss 0.01067076 \tTraining Acuuarcy 52.335% \tValidation Acuuarcy 49.011%\n",
      "Epoch: 12 \tTraining Loss: 0.00967451 \tValidation Loss 0.01035562 \tTraining Acuuarcy 52.787% \tValidation Acuuarcy 50.627%\n",
      "Epoch: 13 \tTraining Loss: 0.00955926 \tValidation Loss 0.01059679 \tTraining Acuuarcy 53.022% \tValidation Acuuarcy 50.432%\n",
      "Epoch: 14 \tTraining Loss: 0.00953506 \tValidation Loss 0.01047931 \tTraining Acuuarcy 53.304% \tValidation Acuuarcy 50.404%\n",
      "Epoch: 15 \tTraining Loss: 0.00948127 \tValidation Loss 0.01051333 \tTraining Acuuarcy 53.929% \tValidation Acuuarcy 49.457%\n",
      "Epoch: 16 \tTraining Loss: 0.00942563 \tValidation Loss 0.01032759 \tTraining Acuuarcy 53.836% \tValidation Acuuarcy 51.045%\n",
      "Epoch: 17 \tTraining Loss: 0.00935630 \tValidation Loss 0.01061288 \tTraining Acuuarcy 54.458% \tValidation Acuuarcy 50.348%\n",
      "Epoch: 18 \tTraining Loss: 0.00930049 \tValidation Loss 0.01071829 \tTraining Acuuarcy 54.650% \tValidation Acuuarcy 48.147%\n",
      "Epoch: 19 \tTraining Loss: 0.00923673 \tValidation Loss 0.01026374 \tTraining Acuuarcy 54.626% \tValidation Acuuarcy 51.323%\n",
      "Epoch: 20 \tTraining Loss: 0.00922638 \tValidation Loss 0.01030828 \tTraining Acuuarcy 55.003% \tValidation Acuuarcy 50.599%\n",
      "Epoch: 21 \tTraining Loss: 0.00917421 \tValidation Loss 0.01019841 \tTraining Acuuarcy 55.452% \tValidation Acuuarcy 49.930%\n",
      "Epoch: 22 \tTraining Loss: 0.00912371 \tValidation Loss 0.01092991 \tTraining Acuuarcy 55.660% \tValidation Acuuarcy 50.098%\n",
      "Epoch: 23 \tTraining Loss: 0.00908133 \tValidation Loss 0.01025880 \tTraining Acuuarcy 56.007% \tValidation Acuuarcy 51.184%\n",
      "Epoch: 24 \tTraining Loss: 0.00904681 \tValidation Loss 0.01029240 \tTraining Acuuarcy 56.038% \tValidation Acuuarcy 50.599%\n",
      "Epoch: 25 \tTraining Loss: 0.00903769 \tValidation Loss 0.01024115 \tTraining Acuuarcy 55.827% \tValidation Acuuarcy 50.933%\n",
      "Epoch: 26 \tTraining Loss: 0.00898597 \tValidation Loss 0.01022799 \tTraining Acuuarcy 56.279% \tValidation Acuuarcy 51.936%\n",
      "Epoch: 27 \tTraining Loss: 0.00896940 \tValidation Loss 0.01053057 \tTraining Acuuarcy 56.394% \tValidation Acuuarcy 50.850%\n",
      "Epoch: 28 \tTraining Loss: 0.00886703 \tValidation Loss 0.01012904 \tTraining Acuuarcy 56.979% \tValidation Acuuarcy 51.268%\n",
      "Epoch: 29 \tTraining Loss: 0.00889126 \tValidation Loss 0.01021668 \tTraining Acuuarcy 56.663% \tValidation Acuuarcy 51.240%\n",
      "Epoch: 30 \tTraining Loss: 0.00892353 \tValidation Loss 0.00995309 \tTraining Acuuarcy 56.623% \tValidation Acuuarcy 52.494%\n",
      "Epoch: 31 \tTraining Loss: 0.00883393 \tValidation Loss 0.01050967 \tTraining Acuuarcy 56.973% \tValidation Acuuarcy 50.181%\n",
      "Epoch: 32 \tTraining Loss: 0.00887652 \tValidation Loss 0.01021205 \tTraining Acuuarcy 56.973% \tValidation Acuuarcy 50.738%\n",
      "Epoch: 33 \tTraining Loss: 0.00885436 \tValidation Loss 0.01027873 \tTraining Acuuarcy 56.988% \tValidation Acuuarcy 52.020%\n",
      "Epoch: 34 \tTraining Loss: 0.00881404 \tValidation Loss 0.01015912 \tTraining Acuuarcy 57.186% \tValidation Acuuarcy 52.354%\n",
      "Epoch: 35 \tTraining Loss: 0.00874614 \tValidation Loss 0.01031295 \tTraining Acuuarcy 57.338% \tValidation Acuuarcy 51.936%\n",
      "Epoch: 36 \tTraining Loss: 0.00872661 \tValidation Loss 0.00995361 \tTraining Acuuarcy 57.843% \tValidation Acuuarcy 53.023%\n",
      "Epoch: 37 \tTraining Loss: 0.00866608 \tValidation Loss 0.01033192 \tTraining Acuuarcy 57.815% \tValidation Acuuarcy 52.438%\n",
      "Epoch: 38 \tTraining Loss: 0.00868255 \tValidation Loss 0.01054615 \tTraining Acuuarcy 57.719% \tValidation Acuuarcy 51.045%\n",
      "Epoch: 39 \tTraining Loss: 0.00862834 \tValidation Loss 0.00992901 \tTraining Acuuarcy 58.378% \tValidation Acuuarcy 53.553%\n",
      "Epoch: 40 \tTraining Loss: 0.00863795 \tValidation Loss 0.00996261 \tTraining Acuuarcy 58.115% \tValidation Acuuarcy 52.215%\n",
      "Epoch: 41 \tTraining Loss: 0.00864726 \tValidation Loss 0.01003231 \tTraining Acuuarcy 58.096% \tValidation Acuuarcy 52.800%\n",
      "Epoch: 42 \tTraining Loss: 0.00861216 \tValidation Loss 0.01023090 \tTraining Acuuarcy 58.236% \tValidation Acuuarcy 51.797%\n",
      "Epoch: 43 \tTraining Loss: 0.00857516 \tValidation Loss 0.01016312 \tTraining Acuuarcy 58.329% \tValidation Acuuarcy 52.020%\n",
      "Epoch: 44 \tTraining Loss: 0.00850415 \tValidation Loss 0.01033502 \tTraining Acuuarcy 58.598% \tValidation Acuuarcy 52.800%\n",
      "Epoch: 45 \tTraining Loss: 0.00859639 \tValidation Loss 0.01002675 \tTraining Acuuarcy 58.180% \tValidation Acuuarcy 51.909%\n",
      "Epoch: 46 \tTraining Loss: 0.00855468 \tValidation Loss 0.01040063 \tTraining Acuuarcy 58.254% \tValidation Acuuarcy 51.686%\n",
      "Epoch: 47 \tTraining Loss: 0.00852250 \tValidation Loss 0.01007381 \tTraining Acuuarcy 58.877% \tValidation Acuuarcy 52.466%\n",
      "Epoch: 48 \tTraining Loss: 0.00844900 \tValidation Loss 0.00996472 \tTraining Acuuarcy 58.917% \tValidation Acuuarcy 51.797%\n",
      "Epoch: 49 \tTraining Loss: 0.00845722 \tValidation Loss 0.01029545 \tTraining Acuuarcy 59.013% \tValidation Acuuarcy 52.382%\n",
      "Epoch: 50 \tTraining Loss: 0.00849744 \tValidation Loss 0.00990622 \tTraining Acuuarcy 58.926% \tValidation Acuuarcy 53.162%\n",
      "Epoch: 51 \tTraining Loss: 0.00853085 \tValidation Loss 0.01033530 \tTraining Acuuarcy 58.682% \tValidation Acuuarcy 52.744%\n",
      "Epoch: 52 \tTraining Loss: 0.00844975 \tValidation Loss 0.01021884 \tTraining Acuuarcy 59.196% \tValidation Acuuarcy 51.853%\n",
      "Epoch: 53 \tTraining Loss: 0.00848045 \tValidation Loss 0.00996624 \tTraining Acuuarcy 58.852% \tValidation Acuuarcy 52.912%\n",
      "Epoch: 54 \tTraining Loss: 0.00848406 \tValidation Loss 0.01022525 \tTraining Acuuarcy 58.694% \tValidation Acuuarcy 52.104%\n",
      "Epoch: 55 \tTraining Loss: 0.00840047 \tValidation Loss 0.00983304 \tTraining Acuuarcy 59.313% \tValidation Acuuarcy 52.744%\n",
      "Epoch: 56 \tTraining Loss: 0.00838617 \tValidation Loss 0.00998738 \tTraining Acuuarcy 59.220% \tValidation Acuuarcy 53.218%\n",
      "Epoch: 57 \tTraining Loss: 0.00840142 \tValidation Loss 0.01006325 \tTraining Acuuarcy 59.143% \tValidation Acuuarcy 53.107%\n",
      "Epoch: 58 \tTraining Loss: 0.00840392 \tValidation Loss 0.00997756 \tTraining Acuuarcy 59.109% \tValidation Acuuarcy 53.385%\n",
      "Epoch: 59 \tTraining Loss: 0.00840676 \tValidation Loss 0.00999903 \tTraining Acuuarcy 59.131% \tValidation Acuuarcy 51.602%\n",
      "Epoch: 60 \tTraining Loss: 0.00834069 \tValidation Loss 0.01034021 \tTraining Acuuarcy 59.484% \tValidation Acuuarcy 51.741%\n",
      "Epoch: 61 \tTraining Loss: 0.00839323 \tValidation Loss 0.00980216 \tTraining Acuuarcy 59.298% \tValidation Acuuarcy 53.190%\n",
      "Epoch: 62 \tTraining Loss: 0.00836979 \tValidation Loss 0.01008337 \tTraining Acuuarcy 59.258% \tValidation Acuuarcy 53.218%\n",
      "Epoch: 63 \tTraining Loss: 0.00827712 \tValidation Loss 0.01019795 \tTraining Acuuarcy 60.075% \tValidation Acuuarcy 52.438%\n",
      "Epoch: 64 \tTraining Loss: 0.00831072 \tValidation Loss 0.01024522 \tTraining Acuuarcy 59.499% \tValidation Acuuarcy 51.741%\n",
      "Epoch: 65 \tTraining Loss: 0.00828484 \tValidation Loss 0.01021964 \tTraining Acuuarcy 59.437% \tValidation Acuuarcy 52.884%\n",
      "Epoch: 66 \tTraining Loss: 0.00826396 \tValidation Loss 0.01046847 \tTraining Acuuarcy 59.713% \tValidation Acuuarcy 52.522%\n",
      "Epoch: 67 \tTraining Loss: 0.00820876 \tValidation Loss 0.00989281 \tTraining Acuuarcy 60.155% \tValidation Acuuarcy 52.633%\n",
      "Epoch: 68 \tTraining Loss: 0.00831720 \tValidation Loss 0.00990136 \tTraining Acuuarcy 59.837% \tValidation Acuuarcy 52.522%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 \tTraining Loss: 0.00831771 \tValidation Loss 0.00995067 \tTraining Acuuarcy 59.576% \tValidation Acuuarcy 52.020%\n",
      "Epoch: 70 \tTraining Loss: 0.00823084 \tValidation Loss 0.00997070 \tTraining Acuuarcy 59.998% \tValidation Acuuarcy 52.689%\n",
      "Epoch: 71 \tTraining Loss: 0.00821664 \tValidation Loss 0.01013743 \tTraining Acuuarcy 60.121% \tValidation Acuuarcy 53.915%\n",
      "Epoch: 72 \tTraining Loss: 0.00824507 \tValidation Loss 0.01015848 \tTraining Acuuarcy 60.146% \tValidation Acuuarcy 52.884%\n",
      "Epoch: 73 \tTraining Loss: 0.00824519 \tValidation Loss 0.01011650 \tTraining Acuuarcy 59.843% \tValidation Acuuarcy 51.853%\n",
      "Epoch: 74 \tTraining Loss: 0.00822327 \tValidation Loss 0.00992755 \tTraining Acuuarcy 60.131% \tValidation Acuuarcy 52.299%\n",
      "Epoch: 75 \tTraining Loss: 0.00822959 \tValidation Loss 0.00984198 \tTraining Acuuarcy 60.205% \tValidation Acuuarcy 52.354%\n",
      "Epoch: 76 \tTraining Loss: 0.00821876 \tValidation Loss 0.00992809 \tTraining Acuuarcy 60.016% \tValidation Acuuarcy 53.608%\n",
      "Epoch: 77 \tTraining Loss: 0.00811916 \tValidation Loss 0.00994933 \tTraining Acuuarcy 60.629% \tValidation Acuuarcy 53.441%\n",
      "Epoch: 78 \tTraining Loss: 0.00817550 \tValidation Loss 0.01011698 \tTraining Acuuarcy 60.372% \tValidation Acuuarcy 53.608%\n",
      "Epoch: 79 \tTraining Loss: 0.00819933 \tValidation Loss 0.00999420 \tTraining Acuuarcy 60.431% \tValidation Acuuarcy 52.800%\n",
      "Epoch: 80 \tTraining Loss: 0.00815400 \tValidation Loss 0.00991657 \tTraining Acuuarcy 60.459% \tValidation Acuuarcy 52.466%\n",
      "Epoch: 81 \tTraining Loss: 0.00814686 \tValidation Loss 0.00980853 \tTraining Acuuarcy 60.651% \tValidation Acuuarcy 53.469%\n",
      "Epoch: 82 \tTraining Loss: 0.00813062 \tValidation Loss 0.01022117 \tTraining Acuuarcy 60.400% \tValidation Acuuarcy 51.797%\n",
      "Epoch: 83 \tTraining Loss: 0.00805313 \tValidation Loss 0.01053558 \tTraining Acuuarcy 61.112% \tValidation Acuuarcy 52.967%\n",
      "Epoch: 84 \tTraining Loss: 0.00819964 \tValidation Loss 0.01054077 \tTraining Acuuarcy 60.409% \tValidation Acuuarcy 53.330%\n",
      "Epoch: 85 \tTraining Loss: 0.00808427 \tValidation Loss 0.01014412 \tTraining Acuuarcy 60.793% \tValidation Acuuarcy 52.410%\n",
      "Epoch: 86 \tTraining Loss: 0.00811340 \tValidation Loss 0.01029426 \tTraining Acuuarcy 60.682% \tValidation Acuuarcy 51.658%\n",
      "Epoch: 87 \tTraining Loss: 0.00811751 \tValidation Loss 0.01004977 \tTraining Acuuarcy 60.577% \tValidation Acuuarcy 53.775%\n",
      "Epoch: 88 \tTraining Loss: 0.00802187 \tValidation Loss 0.01020542 \tTraining Acuuarcy 61.165% \tValidation Acuuarcy 52.633%\n",
      "Epoch: 89 \tTraining Loss: 0.00806624 \tValidation Loss 0.01004314 \tTraining Acuuarcy 60.880% \tValidation Acuuarcy 52.187%\n",
      "Epoch: 90 \tTraining Loss: 0.00804519 \tValidation Loss 0.01031177 \tTraining Acuuarcy 61.090% \tValidation Acuuarcy 53.580%\n",
      "Epoch: 91 \tTraining Loss: 0.00808855 \tValidation Loss 0.00995434 \tTraining Acuuarcy 60.768% \tValidation Acuuarcy 53.469%\n",
      "Epoch: 92 \tTraining Loss: 0.00806886 \tValidation Loss 0.01003023 \tTraining Acuuarcy 60.821% \tValidation Acuuarcy 53.831%\n",
      "Epoch: 93 \tTraining Loss: 0.00799382 \tValidation Loss 0.01004733 \tTraining Acuuarcy 61.128% \tValidation Acuuarcy 52.800%\n",
      "Epoch: 94 \tTraining Loss: 0.00800923 \tValidation Loss 0.00993220 \tTraining Acuuarcy 61.555% \tValidation Acuuarcy 52.187%\n",
      "Epoch: 95 \tTraining Loss: 0.00802723 \tValidation Loss 0.01024983 \tTraining Acuuarcy 61.276% \tValidation Acuuarcy 52.132%\n",
      "Epoch: 96 \tTraining Loss: 0.00807159 \tValidation Loss 0.01017036 \tTraining Acuuarcy 61.007% \tValidation Acuuarcy 52.522%\n",
      "Epoch: 97 \tTraining Loss: 0.00804390 \tValidation Loss 0.00990278 \tTraining Acuuarcy 61.159% \tValidation Acuuarcy 53.970%\n",
      "Epoch: 98 \tTraining Loss: 0.00797314 \tValidation Loss 0.01011774 \tTraining Acuuarcy 61.224% \tValidation Acuuarcy 53.051%\n",
      "Epoch: 99 \tTraining Loss: 0.00799602 \tValidation Loss 0.01020241 \tTraining Acuuarcy 61.332% \tValidation Acuuarcy 53.302%\n",
      "Epoch: 100 \tTraining Loss: 0.00792577 \tValidation Loss 0.00999546 \tTraining Acuuarcy 61.245% \tValidation Acuuarcy 54.110%\n",
      "===================================Training Finished===================================\n"
     ]
    }
   ],
   "source": [
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'DeepEmotion_trainded2.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "batchsize = 128\n",
    "\n",
    "\n",
    "net = Deep_Emotion()  #creating model by calling deepEmotion.py\n",
    "net.to(device)  #moving it to GPU\n",
    "print(\"Model archticture: \", net)\n",
    "traincsv_file = 'data'+'/'+'train.csv'\n",
    "validationcsv_file = 'data'+'/'+'val.csv'\n",
    "train_img_dir = 'data'+'/'+'train/'\n",
    "validation_img_dir = 'data'+'/'+'val/'\n",
    "\n",
    "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optmizer= optim.Adam(net.parameters(),lr= lr)\n",
    "Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f00831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep_Emotion(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=810, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=7, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_emotion import Deep_Emotion\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net=Deep_Emotion()\n",
    "net.load_state_dict(torch.load('DeepEmotion_trainded1.pt'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e26c310",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot open webcam",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     cap\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot open webcam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     ret,frame\u001b[38;5;241m=\u001b[39mcap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot open webcam"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "path='haarcascade_frontalface_default.xml'\n",
    "font_scale=1.5\n",
    "font=cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "rectangle_bgr=(255,255,255)\n",
    "\n",
    "img=np.zeros((500,500))\n",
    "text='Some text '\n",
    "(text_width,text_height)=cv2.getTextSize(text,font,fontScale=font_scale,thickness=1)[0]\n",
    "\n",
    "text_offset_x=10\n",
    "text_offset_y=img.shape[0]-25\n",
    "\n",
    "box_coords=((text_offset_x,text_offset_y),(text_offset_x+text_width+2,text_offset_y-text_height-2))\n",
    "cv2.rectangle(img,box_coords[0],box_coords[1],rectangle_bgr,cv2.FILLED)\n",
    "cv2.putText(img,text,(text_offset_x,text_offset_y),font,fontScale=font_scale,color=(0,0,0),thickness=1)\n",
    "\n",
    "cap=cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    cap=cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError('Cannot open webcam')\n",
    "    \n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    faceCascade = cv2.CascadeClassifier(path)\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "    for x, y, w, h in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            facess = faceCascade.detectMultiScale(roi_gray)\n",
    "\n",
    "            if len(facess)==0:\n",
    "                print(\"face not detected\")\n",
    "            else:\n",
    "                for (ex,ey,ew,eh) in facess:\n",
    "                    \n",
    "                     face_roi=roi_color[ey:ey+eh,ex:ex+ew]\n",
    "                    \n",
    "    graytemp=cv2.cvtColor(face_roi,cv2.COLOR_BGR2GRAY)\n",
    "    final_image=cv2.resize(graytemp,(48,48))\n",
    "    final_image=np.expand_dims(final_image,axis=0)\n",
    "    final_image=np.expand_dims(final_image,axis=0)\n",
    "    final_image=final_image/255.0\n",
    "    dataa=torch.from_numpy(final_image)\n",
    "    dataa=dataa.type(torch.FloatTensor)\n",
    "    dataa=dataa.to(device)\n",
    "    outputs=net(dataa)\n",
    "    Pred=F.softmax(outputs,dim=1)\n",
    "    Predictions=torch.argmax(Pred)\n",
    "    print(cv2.FONT_HERSHEY_SIMPLEXIM)\n",
    "    \n",
    "    font_scale=1.5\n",
    "    font=cv2.FONT_HERSHEY_PLAIN\n",
    "    \n",
    "    if((Predictions==0)):\n",
    "        status='Angry'\n",
    "        \n",
    "        x1,y1,w1,h1=0,0,175,75\n",
    "        \n",
    "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
    "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
    "        \n",
    "        \n",
    "    elif((Predictions==1)):\n",
    "        status='Disgust'\n",
    "        \n",
    "        x1,y1,w1,h1=0,0,175,75\n",
    "        \n",
    "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
    "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
    "        \n",
    "    elif((Predictions==2)):\n",
    "        status='Fear'\n",
    "        \n",
    "        x1,y1,w1,h1=0,0,175,75\n",
    "        \n",
    "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
    "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
    "        \n",
    "    elif ((Predictions==3)):\n",
    "        status='Happy'\n",
    "        \n",
    "        x1,y1,w1,h1=0,0,175,75\n",
    "        \n",
    "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
    "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
    "        \n",
    "    elif((Predictions==4)):\n",
    "        status='Sad'\n",
    "        \n",
    "        x1,y1,w1,h1=0,0,175,75\n",
    "        \n",
    "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
    "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
    "        \n",
    "    elif((Predictions==5)):\n",
    "        status='Suprise'\n",
    "        \n",
    "        x1,y1,w1,h1=0,0,175,75\n",
    "        \n",
    "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
    "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
    "        \n",
    "    elif((Predictions==6)):\n",
    "        status='Natural'\n",
    "        \n",
    "        x1,y1,w1,h1=0,0,175,75\n",
    "        \n",
    "        cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,0,0),-1)\n",
    "        cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "        cv2.putText(frame,status,(100,150),font,3,(0,0,255),2,cv2.LINE_4)\n",
    "        \n",
    "        cv2.imsho('face emotion recognization',frame)\n",
    "        if cv2.waitKey(2)&0xFF==ord('q'):\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619aab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
